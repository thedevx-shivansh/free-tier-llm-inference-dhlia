{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T15:21:40.049361Z","iopub.execute_input":"2026-01-31T15:21:40.050160Z","iopub.status.idle":"2026-01-31T15:21:40.530077Z","shell.execute_reply.started":"2026-01-31T15:21:40.050125Z","shell.execute_reply":"2026-01-31T15:21:40.529153Z"}},"outputs":[{"name":"stdout","text":"Sat Jan 31 15:21:40 2026       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n+-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   39C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   40C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Environment:\n- Platform: Kaggle Notebooks (Free Tier)\n- GPU: NVIDIA Tesla T4 (16GB VRAM)\n- Session Type: Ephemeral\n","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport os\nimport sys\nimport time\nimport shutil\nfrom getpass import getpass\n\ndef main():\n    print(\"=\" * 65)\n    print(\"  SOVEREIGN HYBRID AI - KAGGLE DEPLOYMENT v1.0\")\n    print(\"  Estimated time: 15-25 minutes\")\n    print(\"=\" * 65)\n\n    # Get token first (before any work)\n    print(\"\\nüîê Enter your Ngrok auth token:\")\n    print(\"   (Get it from: https://dashboard.ngrok.com/get-started/your-authtoken)\")\n    NGROK_TOKEN = getpass(\"   Token: \")\n    \n    if len(NGROK_TOKEN) < 20:\n        print(\"   ‚úó Invalid token. Please get a valid token from ngrok.com\")\n        return\n    \n    print(\"\\n\" + \"-\" * 65)\n\n    # =========================================================================\n    # STEP 1: SYSTEM DEPENDENCIES\n    # =========================================================================\n    print(\"\\n‚öôÔ∏è [1/7] Installing system dependencies...\")\n    \n    subprocess.run([\"apt-get\", \"update\", \"-qq\"], capture_output=True)\n    \n    # Install zstd - CRITICAL for Ollama extraction\n    result = subprocess.run(\n        [\"apt-get\", \"install\", \"-y\", \"-qq\", \"zstd\"],\n        capture_output=True\n    )\n    \n    # Verify zstd is installed\n    if shutil.which(\"zstd\"):\n        print(\"   ‚úì zstd installed\")\n    else:\n        print(\"   ‚úó zstd installation failed!\")\n        return\n    \n    # Install fuser for port management\n    subprocess.run([\"apt-get\", \"install\", \"-y\", \"-qq\", \"psmisc\"], capture_output=True)\n    print(\"   ‚úì System dependencies ready\")\n\n    # =========================================================================\n    # STEP 2: PYTHON PACKAGES (with all fixes)\n    # =========================================================================\n    print(\"\\nüì¶ [2/7] Installing Python packages (3-5 minutes)...\")\n    \n    # Upgrade pip\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\", \"-q\"], \n                   capture_output=True)\n    print(\"   ‚úì pip upgraded\")\n    \n    # Install pyngrok\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pyngrok\", \"-q\"], \n                   capture_output=True)\n    print(\"   ‚úì pyngrok installed\")\n    \n    # KAGGLE: May not have cupy issues, but remove just in case\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"cupy-cuda12x\", \"cupy\", \"-y\"], \n                   capture_output=True)\n    print(\"   ‚úì Removed potential conflicting packages\")\n    \n    # Install open-webui\n    result = subprocess.run(\n        [sys.executable, \"-m\", \"pip\", \"install\", \"open-webui\", \"-q\"],\n        capture_output=True, text=True\n    )\n    print(\"   ‚úì open-webui installed\")\n    \n    # Fix alembic (PriorityDispatchResult error)\n    subprocess.run([\n        sys.executable, \"-m\", \"pip\", \"install\", \n        \"alembic>=1.15.0\", \"--force-reinstall\", \"-q\"\n    ], capture_output=True)\n    print(\"   ‚úì alembic fixed\")\n    \n    # Fix transformers (PreTrainedModel error)\n    subprocess.run([\n        sys.executable, \"-m\", \"pip\", \"install\", \n        \"transformers>=4.40.0\", \"--force-reinstall\", \"-q\"\n    ], capture_output=True)\n    print(\"   ‚úì transformers fixed\")\n    \n    # Fix srsly (cupy dependency chain)\n    subprocess.run([\n        sys.executable, \"-m\", \"pip\", \"install\", \n        \"srsly\", \"--force-reinstall\", \"-q\"\n    ], capture_output=True)\n    print(\"   ‚úì srsly fixed\")\n\n    # =========================================================================\n    # STEP 3: INSTALL OLLAMA\n    # =========================================================================\n    print(\"\\nü¶ô [3/7] Installing Ollama...\")\n    \n    OLLAMA_URL = \"https://ollama.com/download/ollama-linux-amd64.tar.zst\"\n    \n    # Clean previous installation\n    subprocess.run([\"rm\", \"-rf\", \"/usr/lib/ollama\", \"/usr/bin/ollama\"], capture_output=True)\n    \n    # Method 1: Pipe directly\n    result = subprocess.run(\n        f'curl -fsSL \"{OLLAMA_URL}\" | tar -x --zstd -C /usr',\n        shell=True, capture_output=True, text=True, timeout=300\n    )\n    \n    # If failed, try Method 2: Download then extract\n    if not shutil.which(\"ollama\"):\n        print(\"   ‚Üí Primary method failed, trying alternative...\")\n        subprocess.run([\n            \"curl\", \"-fsSL\", \"-o\", \"/tmp/ollama.tar.zst\", OLLAMA_URL\n        ], capture_output=True, timeout=300)\n        \n        subprocess.run([\n            \"zstd\", \"-d\", \"/tmp/ollama.tar.zst\", \"-o\", \"/tmp/ollama.tar\"\n        ], capture_output=True)\n        \n        subprocess.run([\n            \"tar\", \"-xf\", \"/tmp/ollama.tar\", \"-C\", \"/usr\"\n        ], capture_output=True)\n        \n        # Cleanup\n        subprocess.run([\"rm\", \"-f\", \"/tmp/ollama.tar.zst\", \"/tmp/ollama.tar\"], capture_output=True)\n    \n    # Verify\n    if shutil.which(\"ollama\"):\n        version = subprocess.run([\"ollama\", \"-v\"], capture_output=True, text=True)\n        print(f\"   ‚úì Ollama installed\")\n    else:\n        print(\"   ‚úó Ollama installation failed!\")\n        print(\"   This might be a temporary network issue. Try again in a few minutes.\")\n        return\n    \n    # KAGGLE: Use persistent storage for Ollama models\n    OLLAMA_HOME = \"/kaggle/working/.ollama\"\n    os.makedirs(OLLAMA_HOME, exist_ok=True)\n    os.environ[\"OLLAMA_MODELS\"] = OLLAMA_HOME\n\n    # =========================================================================\n    # STEP 4: START OLLAMA SERVER\n    # =========================================================================\n    print(\"\\nüöÄ [4/7] Starting Ollama server...\")\n    \n    env = os.environ.copy()\n    env[\"OLLAMA_HOST\"] = \"0.0.0.0:11434\"\n    env[\"OLLAMA_ORIGINS\"] = \"*\"\n    env[\"OLLAMA_MODELS\"] = OLLAMA_HOME  # KAGGLE: Persist models\n    \n    subprocess.Popen(\n        [\"ollama\", \"serve\"],\n        env=env,\n        stdout=subprocess.DEVNULL,\n        stderr=subprocess.DEVNULL,\n        start_new_session=True\n    )\n    \n    # Wait for Ollama to be ready\n    import requests\n    for i in range(30):\n        try:\n            requests.get(\"http://localhost:11434\", timeout=2)\n            print(\"   ‚úì Ollama server running\")\n            break\n        except:\n            time.sleep(2)\n    else:\n        print(\"   ‚úó Ollama failed to start\")\n        return\n\n    # =========================================================================\n    # STEP 5: PULL AI MODELS\n    # =========================================================================\n    print(\"\\nüì• [5/7] Pulling AI models (this takes 10-20 minutes)...\")\n    print(\"        Each model is 4-5 GB. Please be patient.\\n\")\n    print(\"        ‚≠ê KAGGLE BONUS: Models save to /kaggle/working/.ollama\")\n    print(\"           Download once, reuse in future sessions!\\n\")\n    \n    MODELS = [\n        (\"deepseek-r1:8b\", \"Reasoning & Logic\"),\n        (\"qwen2.5-coder:7b\", \"Code Generation\"),\n        (\"dolphin-mistral\", \"Uncensored/Creative\"),\n    ]\n    \n    for i, (model, description) in enumerate(MODELS, 1):\n        print(f\"   [{i}/{len(MODELS)}] Pulling {model} ({description})...\")\n        result = subprocess.run(\n            [\"ollama\", \"pull\", model],\n            capture_output=True, text=True, timeout=1800,\n            env=env\n        )\n        if result.returncode == 0:\n            print(f\"   ‚úì {model} ready\")\n        else:\n            print(f\"   ‚ö† {model} may have issues: {result.stderr[:100]}\")\n\n    # =========================================================================\n    # STEP 6: START OPEN WEBUI\n    # =========================================================================\n    print(\"\\nüåê [6/7] Starting Open WebUI...\")\n    \n    # Kill any existing processes on port 8080\n    subprocess.run(\"fuser -k 8080/tcp 2>/dev/null\", shell=True, capture_output=True)\n    time.sleep(2)\n    \n    import secrets\n    \n    # KAGGLE: Use persistent storage for WebUI data\n    WEBUI_DATA_DIR = \"/kaggle/working/webui_data\"\n    \n    webui_env = os.environ.copy()\n    webui_env.update({\n        \"WEBUI_SECRET_KEY\": secrets.token_urlsafe(32),\n        \"WEBUI_AUTH\": \"False\",\n        \"CORS_ALLOW_ORIGIN\": \"*\",\n        \"OLLAMA_BASE_URL\": \"http://localhost:11434\",\n        \"ENABLE_SIGNUP\": \"False\",\n        \"DATA_DIR\": WEBUI_DATA_DIR,\n    })\n    \n    os.makedirs(WEBUI_DATA_DIR, exist_ok=True)\n    \n    webui_proc = subprocess.Popen(\n        [\"open-webui\", \"serve\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"],\n        env=webui_env,\n        stdout=subprocess.DEVNULL,\n        stderr=subprocess.DEVNULL,\n        start_new_session=True\n    )\n    \n    print(f\"   ‚úì Started (PID: {webui_proc.pid})\")\n    print(\"   ‚Üí Waiting for database migrations...\")\n    \n    # Wait for WebUI to be ready\n    for i in range(90):\n        # Check if process died\n        if webui_proc.poll() is not None:\n            print(\"   ‚úó Open WebUI crashed!\")\n            return\n        \n        try:\n            r = requests.get(\"http://localhost:8080\", timeout=2)\n            if r.status_code in [200, 302, 307]:\n                print(f\"   ‚úì Open WebUI ready!\")\n                break\n        except:\n            pass\n        \n        if i % 15 == 0 and i > 0:\n            print(f\"     ... still initializing ({i*2}s)\")\n        time.sleep(2)\n    else:\n        print(\"   ‚úó Open WebUI timeout\")\n        return\n\n    # =========================================================================\n    # STEP 7: CREATE PUBLIC TUNNEL\n    # =========================================================================\n    print(\"\\nüåç [7/7] Creating public tunnel...\")\n    \n    from pyngrok import ngrok, conf\n    \n    conf.get_default().auth_token = NGROK_TOKEN\n    ngrok.kill()  # Kill any existing tunnels\n    time.sleep(2)\n    \n    try:\n        # CRITICAL: host_header=\"rewrite\" fixes 404 errors\n        tunnel = ngrok.connect(\n            8080,\n            \"http\",\n            bind_tls=True,\n            host_header=\"rewrite\"\n        )\n        public_url = tunnel.public_url\n        print(\"   ‚úì Tunnel established!\")\n    except Exception as e:\n        print(f\"   ‚úó Tunnel failed: {e}\")\n        return\n\n    # =========================================================================\n    # SUCCESS!\n    # =========================================================================\n    print(\"\\n\" + \"=\" * 65)\n    print(\"  üéâ SOVEREIGN HYBRID AI SYSTEM IS ONLINE!\")\n    print(\"=\" * 65)\n    print()\n    print(f\"  üîó ACCESS URL: {public_url}\")\n    print()\n    print(\"  üß† AVAILABLE MODELS:\")\n    print(\"     ‚Ä¢ deepseek-r1:8b     ‚Üí Reasoning & Logic\")\n    print(\"     ‚Ä¢ qwen2.5-coder:7b   ‚Üí Code Generation\")\n    print(\"     ‚Ä¢ dolphin-mistral    ‚Üí Uncensored/Creative\")\n    print()\n    print(\"  ‚≠ê KAGGLE ADVANTAGES:\")\n    print(\"     ‚Ä¢ Models saved to /kaggle/working/.ollama (persistent!)\")\n    print(\"     ‚Ä¢ Data saved to /kaggle/working/webui_data\")\n    print(\"     ‚Ä¢ More stable than Colab - less frequent disconnections\")\n    print()\n    print(\"  ‚ö†Ô∏è  IMPORTANT:\")\n    print(\"     ‚Ä¢ Keep this cell running to maintain access\")\n    print(\"     ‚Ä¢ Session lasts much longer than Colab\")\n    print(\"     ‚Ä¢ Bookmark the URL for this session\")\n    print()\n    print(\"=\" * 65)\n\n    # Keep the script running to maintain the tunnel\n    try:\n        while True:\n            time.sleep(60)\n            # Optional: Add health check\n            try:\n                requests.get(\"http://localhost:8080\", timeout=5)\n            except:\n                print(\"‚ö† WebUI health check failed\")\n    except KeyboardInterrupt:\n        print(\"\\nüõë Shutting down...\")\n        ngrok.kill()\n        print(\"   ‚úì Tunnel closed\")\n        print(\"   ‚úì Shutdown complete\")\n\n# Run the deployment\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T15:23:25.487390Z","iopub.execute_input":"2026-01-31T15:23:25.488280Z","iopub.status.idle":"2026-01-31T16:26:53.365018Z","shell.execute_reply.started":"2026-01-31T15:23:25.488232Z","shell.execute_reply":"2026-01-31T16:26:53.364138Z"}},"outputs":[{"name":"stdout","text":"=================================================================\n  SOVEREIGN HYBRID AI - KAGGLE DEPLOYMENT v1.0\n  Estimated time: 15-25 minutes\n=================================================================\n\nüîê Enter your Ngrok auth token:\n   (Get it from: https://dashboard.ngrok.com/get-started/your-authtoken)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"   Token:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"},{"name":"stdout","text":"\n-----------------------------------------------------------------\n\n‚öôÔ∏è [1/7] Installing system dependencies...\n   ‚úì zstd installed\n   ‚úì System dependencies ready\n\nüì¶ [2/7] Installing Python packages (3-5 minutes)...\n   ‚úì pip upgraded\n   ‚úì pyngrok installed\n   ‚úì Removed potential conflicting packages\n   ‚úì open-webui installed\n   ‚úì alembic fixed\n   ‚úì transformers fixed\n   ‚úì srsly fixed\n\nü¶ô [3/7] Installing Ollama...\n   ‚úì Ollama installed\n\nüöÄ [4/7] Starting Ollama server...\n   ‚úì Ollama server running\n\nüì• [5/7] Pulling AI models (this takes 10-20 minutes)...\n        Each model is 4-5 GB. Please be patient.\n\n        ‚≠ê KAGGLE BONUS: Models save to /kaggle/working/.ollama\n           Download once, reuse in future sessions!\n\n   [1/3] Pulling deepseek-r1:8b (Reasoning & Logic)...\n   ‚úì deepseek-r1:8b ready\n   [2/3] Pulling qwen2.5-coder:7b (Code Generation)...\n   ‚úì qwen2.5-coder:7b ready\n   [3/3] Pulling dolphin-mistral (Uncensored/Creative)...\n   ‚úì dolphin-mistral ready\n\nüåê [6/7] Starting Open WebUI...\n   ‚úì Started (PID: 1062)\n   ‚Üí Waiting for database migrations...\n     ... still initializing (30s)\n     ... still initializing (60s)\n   ‚úì Open WebUI ready!\n\nüåç [7/7] Creating public tunnel...\n   ‚úì Tunnel established!                                                                            \n\n=================================================================\n  üéâ SOVEREIGN HYBRID AI SYSTEM IS ONLINE!\n=================================================================\n\n  üîó ACCESS URL: https://chet-navigational-oralee.ngrok-free.dev\n\n  üß† AVAILABLE MODELS:\n     ‚Ä¢ deepseek-r1:8b     ‚Üí Reasoning & Logic\n     ‚Ä¢ qwen2.5-coder:7b   ‚Üí Code Generation\n     ‚Ä¢ dolphin-mistral    ‚Üí Uncensored/Creative\n\n  ‚≠ê KAGGLE ADVANTAGES:\n     ‚Ä¢ Models saved to /kaggle/working/.ollama (persistent!)\n     ‚Ä¢ Data saved to /kaggle/working/webui_data\n     ‚Ä¢ More stable than Colab - less frequent disconnections\n\n  ‚ö†Ô∏è  IMPORTANT:\n     ‚Ä¢ Keep this cell running to maintain access\n     ‚Ä¢ Session lasts much longer than Colab\n     ‚Ä¢ Bookmark the URL for this session\n\n=================================================================\n\nüõë Shutting down...\n   ‚úì Tunnel closed\n   ‚úì Shutdown complete\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"Note:\nThis notebook serves as an execution log for empirical validation.\nThe full reference deployment script is maintained in the public GitHub\nrepository and represents the authoritative reproducible implementation.\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}